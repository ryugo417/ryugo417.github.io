<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ryugo Morita - Personal Homepage</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>

<nav>
    <a href="#about">About Me</a>
    <a href="#research">Research Experience</a>
    <a href="#internship">Internships</a>
    <a href="#publications">Publications</a>
</nav>

<section id="about">
    <h1>Ryugo Morita</h1>
    <img src="meme.png" alt="Profile Image" class="profile-img">

    <p>
        I am a second-year M.S. student in the Graduate School of Applied Informatics at Hosei University. 
        Currently, I belong to the iMedia Lab led by Assoc. Prof. Jinjia Zhou, where I conduct research on layer-wise image generation and compression in generative models such as diffusion models and GANs. 
        In addition to my work in computer vision, I also engage in HCI research focusing on AI-driven education, exploring how generative AI can enhance learning experiences and cognitive augmentation.
    </p>
    <p>
        <a href="https://scholar.google.com/citations?user=MqJvzUsAAAAJ&hl=ja">Google Scholar</a> | 
        <a href="https://x.com/Oguryu417">Twitter</a> | 
        <a href="https://github.com/ryugo417">Github</a> | 
        <a href="https://www.linkedin.com/in/%E7%AB%9C%E6%A2%A7-%E5%AE%88%E7%94%B0-3391081a6/">LinkedIn</a>
    </p>
</section>

<section id="research">
    <h2>Research Experience</h2>
    <ul>
        <li><strong>SDS DFKI, Germany</strong> (Dec 2023 – Dec 2024)<br>
            Advisor: Prof. Andreas Dengel
            <ul>
                <li>Training-free Chroma Key Content Generation Diffusion Model (CVPR2025)</li>
                <li>Edge-based Denoising Image Compression (EUSIPCO2024)</li>
                <li>GenAIReading (AHs2025)</li>
            </ul>
        </li>
        <li><strong>iMedia Lab, Hosei University</strong> (Apr 2021 – Present)<br>
            Advisor: Assoc. Prof. Jinjia Zhou
            <ul>
                <li>Text-Guided Image Manipulation (WACV2023)</li>
                <li>Background-aware text-to-image synthesis (ICIP2023)</li>
                <li>Background Interpretation-based Foreground Image Synthesize (MIRU2023)</li>
            </ul>
        </li>
    </ul>
</section>

<section id="internship">
    <h2>Internship Experiences</h2>
    <ul>
        <li><strong>SB intuitions</strong> (Mar 2024 – present): Flux model for image editing</li>
        <li><strong>EQUES</strong> (Dec 2023 – present): Research leader for anime generation</li>
        <li><strong>Matsuo Institute</strong> (Oct 2023 – present): Diffusion model for inpainting with Panasonic</li>
        <li><strong>DFKI, Germany</strong> (Dec 2023 – Dec 2024): Diffusion model research</li>
        <li><strong>CyberAgent, Inc.</strong> (Jan 2023 – Dec 2024): Web advertising images generation</li>
        <li><strong>Olympic Broadcasting Services (Paris2024)</strong>: Video editing and translation</li>
        <li><strong>KDDI Research, Inc.</strong> (Dec 2023 – Mar 2024): Edge-based Denoising Image Compression</li>
        <li><strong>DeNA, Inc.</strong> (Aug 2023 – Sep 2023): Baseball data analysis</li>
        <li><strong>Research Center at Hosei University</strong> (Aug 2022 – Dec 2023): Emotion analysis in online classes</li>
        <li><strong>Smart Trade, Inc.</strong> (Aug 2021 – May 2022): Stock price prediction with GAN/LSTM</li>
        <li><strong>Olympic Broadcasting Services (Tokyo2020)</strong>: Video editing and translation</li>
    </ul>
</section>

<section id="publications">
    <h2>Publications</h2>
    <ul>
        <li>
            [1] <strong>TKG-DM: Training-free Chroma Key Content Generation Diffusion Model</strong><br>
            <em><u>Ryugo Morita</u>, 
                <a href="https://stanifrolov.github.io/" target="_blank">Stanislav Frolov</a>, 
                <a href="https://brian-moser.github.io/" target="_blank">Brian Bernhard Moser</a>, 
                <a href="https://scholar.google.com/citations?user=vfxa954AAAAJ&hl=ja" target="_blank">Takahiro Shirakawa</a>,
                <a href="https://sites.google.com/view/ko-watanabe/home" target="_blank">Ko Watanabe</a>, 
                <a href="https://scholar.google.com/citations?user=p3YP0DMAAAAJ&hl=ja" target="_blank">Andreas Dengel</a> and
                <a href="https://scholar.google.com/citations?user=84gWaLUAAAAJ&hl=ja" target="_blank">Jinjia Zhou</a> 
            </em><br>
            <span>CVPR 2025 Highlights (Top 13.5%)</span> 
            [<a href="TKG-DM/TKG-DM.html" target="_blank">Project Page</a>]
            [<a href="https://arxiv.org/abs/2411.15580" target="_blank">Paper</a>]<br>
        </li>
        <li>
            [2] <strong>Bidirectional Learned Facial Animation Codec for Low Bitrate Talking Head Videos</strong><br>
            <em>Riku Takahashi, <u>Ryugo Morita</u>, Fuma Kimishima, Kosuke Iwama and <a href="https://scholar.google.com/citations?user=84gWaLUAAAAJ&hl=ja" target="_blank">Jinjia Zhou</a></em><br>
            <span>DCC 2025</span>
            [<a href="https://arxiv.org/abs/2503.09787v1" target="_blank">Paper</a>]<br>
        </li>
        <li>
            [3] <strong>GenAIReading: Augmenting Human Cognition with Interactive Digital Textbooks Using Large Language Models and Image Generation Models</strong><br>
            <em><u>Ryugo Morita</u>, 
                <a href="https://sites.google.com/view/ko-watanabe/home" target="_blank">Ko Watanabe</a>, 
                <a href="https://scholar.google.com/citations?user=84gWaLUAAAAJ&hl=ja" target="_blank">Jinjia Zhou</a>,
                <a href="https://scholar.google.com/citations?user=p3YP0DMAAAAJ&hl=ja" target="_blank">Andreas Dengel</a>,
                <a href="https://scholar.google.co.jp/citations?user=052bgSAAAAAJ&hl=ja" target="_blank">Shoya Ishimaru</a></em><br>
            <span>AHs 2025</span> 
            [<a href="https://arxiv.org/abs/2503.07463" target="_blank">Paper</a>]<br>
        </li>
        <li>
            [4] <strong>Edge-based Denoising Image Compression</strong><br>
            <em><u>Ryugo Morita</u>, 
                <a href="https://scholar.google.com/citations?user=iIHuJfUAAAAJ&hl=ja" target="_blank">Hitoshi Nishimura</a>,
                <a href="https://sites.google.com/view/ko-watanabe/home" target="_blank">Ko Watanabe</a>, 
                <a href="https://scholar.google.com/citations?user=p3YP0DMAAAAJ&hl=ja" target="_blank">Andreas Dengel</a> and
                <a href="https://scholar.google.com/citations?user=84gWaLUAAAAJ&hl=ja" target="_blank">Jinjia Zhou</a>
            </em><br>
            <span>EUSIPCO 2024</span>
            [<a href="https://arxiv.org/abs/2409.10978" target="_blank">Paper</a>]<br>
        </li>
        <li>
            [5] <strong>Learned Measurement Interpolation for Scalable Compressive Sensing</strong><br>
            <em>Manato Shirai, Fuma Kimishima, 
                <a href="https://scholar.google.com/citations?user=84gWaLUAAAAJ&hl=ja" target="_blank">Jinjia Zhou</a>, <u>Ryugo Morita</u></em><br>
            <span>IJCNN 2024</span>
            [<a href="https://ieeexplore.ieee.org/document/10650162" target="_blank">Paper</a>]<br>

        </li>
        <li>
            [6] <strong>Visual question answering based evaluation metrics for text-to-image generation</strong><br>
            <em>Mizuki Miyamoto, <u>Ryugo Morita</u>, <a href="https://scholar.google.com/citations?user=84gWaLUAAAAJ&hl=ja" target="_blank"></a>Jinjia Zhou</a></em><br>
            <span>ISCAS 2023</span>
            [<a href="https://arxiv.org/abs/2411.10183" target="_blank">Paper</a>]<br>
        </li>
        <li>
            [7] <strong>Block based Adaptive Compressive Sensing with Sampling Rate Control</strong><br>
            <em>Kosuke Iwama, <u>Ryugo Morita</u>, 
            <a href="https://scholar.google.com/citations?user=84gWaLUAAAAJ&hl=ja" target="_blank">Jinjia Zhou</a></em><br>
            <span>ACMM Asia 2023</span>
            [<a href="https://arxiv.org/abs/2411.10200" target="_blank">Paper</a>]<br>
        </li>
        <li>
            [8] <strong>Dynamic Unilateral Dual Learning for Text to Image Synthesis</strong><br>
            <em>
                <a href="https://scholar.google.com.hk/citations?hl=zh-CN&pli=1&user=HX8PE3gAAAAJ" target="_blank">Zhiqiang Zhang</a>, 
                <a href="https://scholar.google.com/citations?user=q-n9vjoAAAAJ&hl=ja" target="_blank">Jiayao Xu</a>, 
                <u>Ryugo Morita</u>, Wenxin Yu, 
                <a href="https://scholar.google.com/citations?user=84gWaLUAAAAJ&hl=ja" target="_blank">Jinjia Zhou</a>
            </em><br>
            <span>ICIP 2023</span>
            [<a href="https://ieeexplore.ieee.org/document/10223128" target="_blank">Paper</a>]<br>
        </li>
        <li>
            [9] <strong>Batinet: Background-aware Text to Image Synthesis and Manipulation Network</strong><br>
            <em>
                <u>Ryugo Morita</u>, 
                <a href="https://scholar.google.com.hk/citations?hl=zh-CN&pli=1&user=HX8PE3gAAAAJ" target="_blank">Zhiqiang Zhang</a>, 
                <a href="https://scholar.google.com/citations?user=84gWaLUAAAAJ&hl=ja" target="_blank">Jinjia Zhou</a>
            </em><br>
            <span>ICIP 2023</span>
            [<a href="https://arxiv.org/abs/2308.05921" target="_blank">Paper</a>]<br>
        </li>
        <li>
            [10] <strong>Interactive Image Manipulation with Complex Text Instructions</strong><br>
            <em>
                <u>Ryugo Morita</u>, 
                <a href="https://scholar.google.com.hk/citations?hl=zh-CN&pli=1&user=HX8PE3gAAAAJ" target="_blank">Zhiqiang Zhang</a>, 
                <a href="https://minhmanho.github.io/" target="_blank">Man M. Ho</a>, 
                <a href="https://scholar.google.com/citations?user=84gWaLUAAAAJ&hl=ja" target="_blank">Jinjia Zhou</a>
            </em><br>
            <span>WACV 2023</span>
            [<a href="https://arxiv.org/abs/2211.15352" target="_blank">Paper</a>]<br>
        </li>
    </ul>
</section>

</body>
</html>
